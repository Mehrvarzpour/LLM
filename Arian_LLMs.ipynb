{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets accelerate torch torchvision --quiet\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Load and preprocess dataset\n",
        "# -----------------------------\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")  # Use a smaller dataset\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # Use a smaller model\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize text with padding and truncation\n",
        "    tokenized = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)  # Reduced max_length\n",
        "    # For T5, labels = input_ids in language modeling tasks\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "train_data = dataset['train'].map(preprocess_function, batched=True)\n",
        "val_data = dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Load T5 Model\n",
        "# -----------------------------\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")  # Use a smaller model\n",
        "model.to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Training Setup\n",
        "# -----------------------------\n",
        "batch_size = 4  # Reduce batch size\n",
        "epochs = 1  # Reduce epochs for faster training\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Training Loop\n",
        "# -----------------------------\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1} | Step {step+1}/{len(train_dataloader)} | Loss: {total_loss / (step+1):.4f}\")\n",
        "\n",
        "    # Validation after each epoch\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Validation Loss: {val_loss / len(val_dataloader):.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Save model and tokenizer\n",
        "# -----------------------------\n",
        "model.save_pretrained(\"./t5_simplified_model\")\n",
        "tokenizer.save_pretrained(\"./t5_simplified_tokenizer\")\n",
        "\n",
        "print(\"Model and tokenizer saved successfully!\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 6: Text Generation Example\n",
        "# -----------------------------\n",
        "prompt = \"The future of AI is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs['input_ids'], max_length=50, num_beams=3, early_stopping=True)  # Reduce generation length\n",
        "print(\"Generated Text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFWHZsIE20FC",
        "outputId": "087f043e-11e1-41e1-9f42-4f7cc7dfac3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1 | Step 100/9180 | Loss: 3.1914\n",
            "Epoch 1 | Step 200/9180 | Loss: 1.9694\n",
            "Epoch 1 | Step 300/9180 | Loss: 1.4776\n",
            "Epoch 1 | Step 400/9180 | Loss: 1.1873\n",
            "Epoch 1 | Step 500/9180 | Loss: 1.0030\n",
            "Epoch 1 | Step 600/9180 | Loss: 0.8722\n",
            "Epoch 1 | Step 700/9180 | Loss: 0.7772\n",
            "Epoch 1 | Step 800/9180 | Loss: 0.7032\n",
            "Epoch 1 | Step 900/9180 | Loss: 0.6429\n"
          ]
        }
      ]
    }
  ]
}